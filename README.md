# Website Error Tracker

Website Error Tracker is a lightweight web application designed to scan and monitor websites for common issues such as HTTP errors, broken links, failed assets, and slow response times. It provides both a simple web interface and REST APIs for tracking scan history and detailed results.

The project is built with **Node.js**, **Express**, **SQLite**, and a minimal **vanilla JavaScript** frontend, focusing on simplicity, performance, and clarity.

---

## ğŸš€ Features

- Scan any website URL and retrieve its HTTP status  
- Measure server response time in milliseconds  
- Detect broken internal links and failed assets (JS, CSS, images)  
- Analyze up to 20 links and assets per scan  
- Persist scan results and history using SQLite  
- View latest scan results and historical data via REST API  
- Simple and clean frontend built without heavy frameworks  

---

## ğŸ§± Architecture Overview

```

WebsiteErrorTracker/
â”‚
â”œâ”€â”€ public/              # Frontend files
â”‚   â”œâ”€â”€ index.html       # Main UI
â”‚   â”œâ”€â”€ app.js           # Frontend logic
â”‚   â””â”€â”€ styles.css       # Styling
â”‚
â”œâ”€â”€ sql/
â”‚   â””â”€â”€ init.sql         # Database schema
â”‚
â”œâ”€â”€ db.js                # SQLite connection and initialization
â”œâ”€â”€ server.js            # Express server and API routes
â”œâ”€â”€ package.json         # Dependencies and scripts
â””â”€â”€ README.md

````

## ğŸ›  Tech Stack

**Frontend**
- HTML
- CSS
- Vanilla JavaScript

**Backend**
- Node.js
- Express

**Database**
- SQLite (WAL mode enabled)

**Utilities**
- Cheerio (HTML parsing)
- node-fetch (HTTP requests)
- dotenv (environment variables)

---

## âš¡ Installation & Setup

### 1ï¸âƒ£ Clone the repository
```bash
git clone https://github.com/esrselin/WebsiteErrorTracker.git
cd WebsiteErrorTracker
````

### 2ï¸âƒ£ Install dependencies

```bash
npm install
```

### 3ï¸âƒ£ Configure environment variables

Create a `.env` file in the project root:

```env
PORT=3000
SCAN_TIMEOUT_MS=10000
```

### 4ï¸âƒ£ Start the application

```bash
npm run dev
```

The application will be available at:

```
http://localhost:3000
```

---

## ğŸŒ Frontend Usage

1. Open the application in your browser
2. Enter a full website URL (including `https://`)
3. Click **Scan**
4. View:

   * Latest scan result
   * Historical scan list
   * Response time, broken links, and asset status

---

## ğŸ”Œ API Endpoints

### Scan a Website

**POST** `/api/scan`

```json
{
  "url": "https://example.com"
}
```

**Response**

```json
{
  "id": 1,
  "url": "https://example.com",
  "status": 200,
  "response_ms": 312,
  "checked_links": 12,
  "broken_links": 1
}
```

---

### Get Scan Details

**GET** `/api/scan/:id`

Returns the scan summary and all checked links/assets.

---

### Get Scan History

**GET** `/api/history?limit=20`

Returns the most recent scans.

---

## ğŸ—„ Database Schema

### `scans`

| Column        | Type    | Description             |
| ------------- | ------- | ----------------------- |
| id            | INTEGER | Primary key             |
| url           | TEXT    | Scanned URL             |
| status        | INTEGER | HTTP status             |
| response_ms   | INTEGER | Response time           |
| checked_links | INTEGER | Number of checked links |
| broken_links  | INTEGER | Number of broken links  |
| created_at    | TEXT    | Timestamp               |

### `scan_details`

| Column  | Type    | Description        |
| ------- | ------- | ------------------ |
| scan_id | INTEGER | Foreign key        |
| link    | TEXT    | Checked URL        |
| status  | INTEGER | HTTP status        |
| ok      | INTEGER | 1 = OK, 0 = Broken |
| kind    | TEXT    | page / asset       |

---

## âš ï¸ Limitations

* Not intended for large-scale crawling
* Limited to 20 links/assets per scan
* No authentication or multi-user support
* Designed for learning, demos, and lightweight monitoring

---

## ğŸ“Œ Future Improvements

* Scheduled scans (cron support)
* Authentication & multi-tenant support
* Export results (CSV / JSON)
* UI improvements and charts
* Configurable crawl depth

---

## ğŸ¤ Contributing

Contributions are welcome!

1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Open a pull request
